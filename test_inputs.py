import torch
from t2v_metrics.models.vqascore_models.clip_t5_model import CLIPT5Model

# âœ… 1ï¸âƒ£ ëª¨ë¸ ë¡œë“œ (ì´ì œ CLIPê³¼ T5 Tokenizerë¥¼ ë¡œë“œí•˜ì§€ ì•ŠìŒ!)
device = "cuda" if torch.cuda.is_available() else "cpu"
vqa_model = CLIPT5Model(model_name='clip-flant5-xxl', device=device)

# âœ… 2ï¸âƒ£ ì €ì¥ëœ Feature ë¡œë“œ
image_features = torch.load("image_features.pt").to(device)  # âœ… (M, 576, 1024)
input_ids = torch.load("input_ids.pt").to(device)  # âœ… (N, sequence_length)
labels = torch.load("labels.pt").to(device)  # âœ… (N, sequence_length)

# # âœ… 3ï¸âƒ£ ì›ë³¸ ë°ì´í„° ì •ì˜ (raw ì´ë¯¸ì§€ ê²½ë¡œ & í…ìŠ¤íŠ¸)
# image_paths = ["images/0.png", "images/1.png"]  # 2ê°œì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
# texts = ["someone talks on the phone angrily while another person sits happily",
#          "someone talks on the phone happily while another person sits angrily"]  # 2ê°œì˜ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸

# âœ… 4ï¸âƒ£ OOM ë°©ì§€ë¥¼ ìœ„í•´ forë¬¸ì„ ì‚¬ìš©í•œ ê°œë³„ ì‹¤í–‰
def compute_vqa_scores(raw_images=None, img_features=None, raw_texts=None, input_ids=None, labels=None):
    """ ë„¤ ê°€ì§€ ì…ë ¥ ë°©ì‹ì— ëŒ€í•´ VQAScoreë¥¼ ê°œë³„ì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ OOM ë°©ì§€ """
    # M = len(image_paths)
    # N = len(texts)
    M = len(image_features.shape[0])
    N = len(input_ids.shape[0])
    scores = torch.zeros((M, N)).to(device)  # ë¹ˆ score í…ì„œ ìƒì„±

    for i in range(M):
        for j in range(N):
            img_feat = None if img_features is None else img_features[i].unsqueeze(0)  # (1, 576, 1024)
            
            # âœ… text_featì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ êµ¬ì„±
            txt_feat = None if input_ids is None else {
                "input_ids": input_ids[j].unsqueeze(0),  # (1, sequence_length)
                "labels": labels[j].unsqueeze(0)  # (1, sequence_length)
            }

            score = vqa_model.forward(
                # images=[raw_images[i]] if raw_images is not None else None,
                image_features=img_feat,
                # texts=[raw_texts[j]] if raw_texts is not None else None,
                text_features=txt_feat,
            )
            scores[i, j] = score.item()

    return scores

# âœ… (1) Raw ì´ë¯¸ì§€ + Raw í…ìŠ¤íŠ¸
# score_1 = compute_vqa_scores(raw_images=image_paths, raw_texts=texts)
# print(f"âœ… (1) Raw Image + Raw Text Score:\n{score_1}")

# # âœ… (2) ì´ë¯¸ì§€ feature + Raw í…ìŠ¤íŠ¸
# score_2 = compute_vqa_scores(img_features=image_features, raw_texts=texts)
# print(f"âœ… (2) Image Feature + Raw Text Score:\n{score_2}")

# # âœ… (3) Raw ì´ë¯¸ì§€ + Tokenized í…ìŠ¤íŠ¸
# score_3 = compute_vqa_scores(raw_images=image_paths, input_ids=input_ids, labels=labels)
# print(f"âœ… (3) Raw Image + Tokenized Text Score:\n{score_3}")

# âœ… (4) ì´ë¯¸ì§€ feature + Tokenized í…ìŠ¤íŠ¸
score_4 = compute_vqa_scores(img_features=image_features, input_ids=input_ids, labels=labels)
print(f"âœ… (4) Image Feature + Tokenized Text Score:\n{score_4}")

# âœ… 5ï¸âƒ£ ëª¨ë“  ì ìˆ˜ ë¹„êµ
# print("\nğŸ” ë¹„êµ ê²°ê³¼:")
# print(f"Raw Image + Raw Text == Image Feature + Raw Text? {'âœ…' if torch.allclose(score_1, score_2) else 'âŒ'}")
# print(f"Raw Image + Raw Text == Raw Image + Tokenized Text? {'âœ…' if torch.allclose(score_1, score_3) else 'âŒ'}")
# print(f"Raw Image + Raw Text == Image Feature + Tokenized Text? {'âœ…' if torch.allclose(score_1, score_4) else 'âŒ'}")

